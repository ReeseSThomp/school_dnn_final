{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comment analysis with NLP with RNN - Final Project\n",
    "\n",
    "Reese Thompson, Feb 16th, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem and Data Analysis\n",
    "\n",
    "This notebook will explore and analyze a set of toxic comments from a Wikipedia data set to train a model capable of identifying types of toxic comments, beyond just a toxic comment.  The target value will ultimately be a probablitily that a comment is indeed toxic.\n",
    "\n",
    "The initial data set includes a large set of comments that have been categorized accordingly.  We will use this data to build an NLP model using both LSTM RNN and GRU RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by evaluating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_raw = pd.read_csv('data/train.csv')\n",
    "\n",
    "train = train_raw.copy()\n",
    "\n",
    "print(\"\\nTrain Data Head:\")\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Data Cleanup\n",
    "\n",
    "We should also verify the distrobution of positive and negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_text = train['comment_text'].isnull().sum()\n",
    "print(f\"Missing text: {missing_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "label_counts = train[labels].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values, palette='viridis')\n",
    "plt.title(\"Distribution of Toxicity Labels\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def simple_clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    return text\n",
    "\n",
    "train['text'] = train['comment_text'].apply(simple_clean)\n",
    "train = train.drop(columns=[\"comment_text\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['id'], axis=1)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_length'] = train['text'].apply(lambda x: len(str(x)))\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train['text_length'], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Comment Text Length\")\n",
    "plt.xlabel(\"Text Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mean_length = train['text_length'].mean()\n",
    "median_length = train['text_length'].median()\n",
    "mode_length = train['text_length'].mode()[0]\n",
    "\n",
    "print(f\"Mean Text Length: {mean_length}\")\n",
    "print(f\"Median Text Length: {median_length}\")\n",
    "print(f\"Mode Text Length: {mode_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short = train[train['text'].str.len() < 10]\n",
    "\n",
    "print(\"Number of comments under 10 characters:\", short.shape[0])\n",
    "\n",
    "short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = train[train['text'].str.len() > 3000]\n",
    "\n",
    "print(\"Number of comments over 3000 characters:\", long.shape[0])\n",
    "\n",
    "long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_95 = train['text_length'].quantile(0.95)\n",
    "print(f\"95th Percentile of Text Length: {length_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['text'].str.len() < 1306]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace('_', ' ').lower()\n",
    "            if synonym != word:\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(comment, n=2):\n",
    "    words = nltk.word_tokenize(comment)\n",
    "    random_word_list = list(set(words))\n",
    "    random.shuffle(random_word_list)\n",
    "    \n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            words = [synonym if word == random_word else word for word in words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_rows = []\n",
    "for idx, row in train[train['identity_hate'] > 0].iterrows():\n",
    "    text = row['text']\n",
    "    for _ in range(15):\n",
    "        new_text = synonym_replacement(text, n=2)\n",
    "        new_row = row.copy()\n",
    "        new_row['text'] = new_text\n",
    "        augmented_rows.append(new_row)\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "train = pd.concat([train, augmented_df], axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_rows = []\n",
    "for idx, row in train[train['severe_toxic'] > 0].iterrows():\n",
    "    text = row['text']\n",
    "    for _ in range(15):\n",
    "        new_text = synonym_replacement(text, n=2)\n",
    "        new_row = row.copy()\n",
    "        new_row['text'] = new_text\n",
    "        augmented_rows.append(new_row)\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "train = pd.concat([train, augmented_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_rows = []\n",
    "for idx, row in train[train['threat'] > 0].iterrows():\n",
    "    text = row['text']\n",
    "    for _ in range(25):\n",
    "        new_text = synonym_replacement(text, n=2)\n",
    "        new_row = row.copy()\n",
    "        new_row['text'] = new_text\n",
    "        augmented_rows.append(new_row)\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_rows)\n",
    "train = pd.concat([train, augmented_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "label_counts = train[labels].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values, palette='viridis')\n",
    "plt.title(\"Distribution of Toxicity Labels\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence = train[labels].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(co_occurrence, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Among Toxicity Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup\n",
    "\n",
    "We will use both LSTM and GRU to test which approach yields the best result.  We will first need to tokenize the text data, using the Keras tokenizer.  In the code below, we set up a temp tokenizer to find the max vocabulary size we should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "\n",
    "X = train['text'].values\n",
    "\n",
    "tokenizer_temp = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer_temp.fit_on_texts(X)\n",
    "\n",
    "word_counts = tokenizer_temp.word_counts\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "total_count = sum(word_counts.values())\n",
    "print(f\"Total word count: {total_count}\")\n",
    "\n",
    "coverage_threshold = 0.97\n",
    "running_count = 0\n",
    "vocab_size_threshold = 0\n",
    "\n",
    "for i, (word, count) in enumerate(sorted_word_counts):\n",
    "    running_count += count\n",
    "    if running_count / total_count >= coverage_threshold:\n",
    "        vocab_size_threshold = i + 1\n",
    "        break\n",
    "\n",
    "print(f\"Coverage threshold of {coverage_threshold} reached at vocab size: {vocab_size_threshold}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the max vocabulary size, we can set up our data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot training curves\n",
    "def plot_training_curves(history, title):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model architecture for the project will be a straight forward LSTM implementation, having an embedding layer feeding into the LSTM layer, with a dropout layer added to prevent overfitting.  This output will then be fed to dense layer for interpreting the LSTM results, with a final dropout layer added.  The final output layer will be a sigmoid activation since this is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Text data as a list of strings\n",
    "train_texts = train['text'].astype(str).tolist()\n",
    "\n",
    "# Label data as a numpy array of shape (num_samples, 6)\n",
    "train_labels = train[labels].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "max_words = 10500   # Maximum vocabulary size\n",
    "max_len = 1350       # Maximum sequence length for padding\n",
    "embedding_dim = 100 # Dimension of embedding vectors\n",
    "\n",
    "# 1. Fit the tokenizer on the training set only\n",
    "tokenizer = Tokenizer(num_words=max_words, lower=True, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 2. Convert text to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "# 3. Pad sequences\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "X_val_padded = pad_sequences(X_val_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# 4. Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "\n",
    "# LSTM layer\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "\n",
    "# Optional Dropout to prevent overfitting\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Dense layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer: 6 units for 6 labels, sigmoid activation for multi-label\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "# 5. Compile the model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train, \n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "\n",
    "plot_training_curves(history, \"Initial Model Accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Hyperparameters\n",
    "\n",
    "The initial model shows a significant difference between the training and validation data, suggesting our model is overfitting.  We will now look to optimize hyperparameters to find the best model parameters for this problem set.  We can also compare the LSTM and GRU models to see if there is any significant difference between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gru_model(embedding_dim, dropout, learning_rate, epochs, batch_size, first_layer_size, second_layer_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_vocab_size, \n",
    "                        output_dim=embedding_dim, \n",
    "                        input_length=max_length))\n",
    "    model.add(GRU(first_layer_size, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(second_layer_size, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_padded, \n",
    "        y_train, \n",
    "        validation_data=(X_val_padded, y_val),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "def test_lstm_model(embedding_dim, dropout, learning_rate, epochs, batch_size, first_layer_size, second_layer_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_vocab_size, \n",
    "                        output_dim=embedding_dim, \n",
    "                        input_length=max_length))\n",
    "    model.add(GRU(first_layer_size, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(second_layer_size, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_padded, \n",
    "        y_train, \n",
    "        validation_data=(X_val_padded, y_val),\n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "\n",
    "    return val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_acc = test_gru_model(128, .3,.001, 25, 64, 100, 50)\n",
    "lstm_acc = test_lstm_model(128, .3,.001, 25, 64, 100, 50)\n",
    "\n",
    "print(f\"LSTM - {lstm_acc}\")\n",
    "print(f\"GRU - {gru_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many hyperparameters to test in a full grid search, so we can perform a search to try to find optimals for each parameter individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hypers = {}\n",
    "\n",
    "embedding_dim_list = [64, 128, 256]\n",
    "dropout_list = [0.2, 0.3, 0.5]\n",
    "learning_rate_list = [0.001, 0.0001, 0.00001]\n",
    "epochs_list = [20, 30, 50]\n",
    "batch_size_list = [32, 64, 128]\n",
    "first_layer_size_list = [64, 128]\n",
    "second_layer_size_list = [32, 64]\n",
    "\n",
    "for emb in embedding_dim_list:\n",
    "    accuracy = test_lstm_model(\n",
    "            embedding_dim=emb,\n",
    "            dropout=.2,\n",
    "            learning_rate=.001,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            first_layer_size=64,\n",
    "            second_layer_size=32\n",
    "        )\n",
    "        \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hypers['embedding_dim'] = emb\n",
    "\n",
    "print(best_hypers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "for drop in dropout_list:\n",
    "    accuracy = test_lstm_model(\n",
    "            embedding_dim=128,\n",
    "            dropout=drop,\n",
    "            learning_rate=.001,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            first_layer_size=64,\n",
    "            second_layer_size=32\n",
    "        )\n",
    "        \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hypers['dropout'] = drop\n",
    "\n",
    "print(best_hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "for learn in learning_rate_list:\n",
    "    accuracy = test_lstm_model(\n",
    "            embedding_dim=128,\n",
    "            dropout=.2,\n",
    "            learning_rate=learn,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            first_layer_size=64,\n",
    "            second_layer_size=32\n",
    "        )\n",
    "        \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hypers['learn'] = learn\n",
    "\n",
    "print(best_hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "for e in epochs_list:\n",
    "    accuracy = test_lstm_model(\n",
    "            embedding_dim=128,\n",
    "            dropout=.2,\n",
    "            learning_rate=.0001,\n",
    "            epochs=e,\n",
    "            batch_size=32,\n",
    "            first_layer_size=64,\n",
    "            second_layer_size=32\n",
    "        )\n",
    "        \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hypers['epochs'] = e\n",
    "\n",
    "print(best_hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "for batch in batch_size_list:\n",
    "    accuracy = test_lstm_model(\n",
    "            embedding_dim=128,\n",
    "            dropout=.2,\n",
    "            learning_rate=.0001,\n",
    "            epochs=20,\n",
    "            batch_size=batch,\n",
    "            first_layer_size=64,\n",
    "            second_layer_size=32\n",
    "        )\n",
    "        \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hypers['batch'] = batch\n",
    "\n",
    "print(best_hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_accuracy = 0\n",
    "for fls in first_layer_size_list:\n",
    "    accuracy = test_lstm_model(\n",
    "            embedding_dim=128,\n",
    "            dropout=.2,\n",
    "            learning_rate=.0001,\n",
    "            epochs=20,\n",
    "            batch_size=128,\n",
    "            first_layer_size=fls,\n",
    "            second_layer_size=32\n",
    "        )\n",
    "        \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hypers['fls'] = batch\n",
    "\n",
    "print(best_hypers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "for sls in second_layer_size_list:\n",
    "    accuracy = test_lstm_model(\n",
    "            embedding_dim=128,\n",
    "            dropout=.2,\n",
    "            learning_rate=.0001,\n",
    "            epochs=20,\n",
    "            batch_size=128,\n",
    "            first_layer_size=128,\n",
    "            second_layer_size=sls\n",
    "        )\n",
    "        \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hypers['sls'] = sls\n",
    "\n",
    "print(best_hypers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review and conclusion\n",
    "\n",
    "We now have a set of optimal parameters for running our LSTM RNN on the Tweet data.  We can now verify our accuarcy for the project to test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_vocab_size, \n",
    "                    output_dim=embedding_dim, \n",
    "                    input_length=max_length))\n",
    "model.add(LSTM(128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded, \n",
    "    y_train, \n",
    "    validation_data=(X_val_padded, y_val),\n",
    "    epochs=num_epochs, \n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_val_padded, y_val, verbose=0)\n",
    "\n",
    "plot_training_curves(history, \"Initial Model Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.predict(X_test_padded)\n",
    "predictions = (predictions >= 0.5).astype(int).flatten()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'target': predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('./submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
